{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Construir un Asistente LLM basado en RAG usando Streamlit y Snowflake Cortex Search\n",
        "\n",
        "*NOTA: Para requisitos previos y otras instrucciones, consulta la [Guía de Inicio Rápido](https://quickstarts.snowflake.com/guide/ask_questions_to_your_own_documents_with_snowflake_cortex_search/index.html#0).*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuración Inicial\n",
        "\n",
        "Crear una base de datos y un schema.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "sql"
        }
      },
      "outputs": [],
      "source": [
        "-- Crear base de datos y schema\n",
        "-- CREATE DATABASE IF NOT EXISTS CC_QUICKSTART_CORTEX_SEARCH_DOCS;\n",
        "-- CREATE SCHEMA IF NOT EXISTS DATA;\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Organizar Documentos y Crear Función de Pre-procesamiento\n",
        "\n",
        "Paso 1. Los documentos PDF de muestra están incluidos en la carpeta `documentos/`. Son los PDFs originales del repositorio de Snowflake (en inglés) que incluyen especificaciones de productos deportivos.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Paso 2. Crear un Stage con Directory Table donde subirás tus documentos.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "sql"
        }
      },
      "outputs": [],
      "source": [
        "-- Crear stage con cifrado y tabla de directorio habilitada\n",
        "CREATE OR REPLACE STAGE docs \n",
        "ENCRYPTION = (TYPE = 'SNOWFLAKE_SSE') \n",
        "DIRECTORY = (ENABLE = true);\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Paso 3. Subir documentos PDF a tu área de staging\n",
        "\n",
        "- Selecciona **Data** en el menú izquierdo\n",
        "- Haz clic en tu base de datos **CC_QUICKSTART_CORTEX_SEARCH_DOCS**\n",
        "- Haz clic en tu schema **DATA**\n",
        "- Haz clic en **Stages** y selecciona **DOCS**\n",
        "- En la esquina superior derecha haz clic en el botón **+Files**\n",
        "- Arrastra y suelta los documentos PDF de la carpeta `documentos/`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Paso 4. Verificar que los archivos se han subido correctamente\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "sql"
        }
      },
      "outputs": [],
      "source": [
        "-- Listar archivos en el stage\n",
        "LS @docs;\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Pre-procesar y Etiquetar Documentos\n",
        "\n",
        "Paso 1. Crear la tabla donde vamos a almacenar los chunks (fragmentos) de cada documento.\n",
        "\n",
        "Vamos a aprovechar las funciones nativas de procesamiento de documentos de Snowflake para preparar los documentos antes de habilitar Cortex Search. También vamos a usar la función Cortex CLASSIFY_TEXT para etiquetar el tipo de documento que se está procesando, de modo que podamos usar esos metadatos para filtrar búsquedas.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "sql"
        }
      },
      "outputs": [],
      "source": [
        "-- Crear tabla temporal para almacenar el texto extraído de los documentos\n",
        "CREATE OR REPLACE TEMPORARY TABLE RAW_TEXT AS\n",
        "SELECT \n",
        "    RELATIVE_PATH,\n",
        "    SIZE,\n",
        "    FILE_URL,\n",
        "    BUILD_SCOPED_FILE_URL(@docs, relative_path) AS SCOPED_FILE_URL,\n",
        "    TO_VARCHAR (\n",
        "        SNOWFLAKE.CORTEX.PARSE_DOCUMENT (\n",
        "            '@docs',\n",
        "            RELATIVE_PATH,\n",
        "            {'mode': 'LAYOUT'}\n",
        "        ):content\n",
        "    ) AS EXTRACTED_LAYOUT \n",
        "FROM \n",
        "    DIRECTORY('@docs');\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "sql"
        }
      },
      "outputs": [],
      "source": [
        "-- Crear tabla para almacenar los chunks de texto\n",
        "CREATE OR REPLACE TABLE DOCS_CHUNKS_TABLE ( \n",
        "    RELATIVE_PATH STRING, -- Ruta relativa al archivo\n",
        "    SIZE NUMBER(38,0), -- Tamaño del archivo\n",
        "    FILE_URL STRING, -- URL para el archivo\n",
        "    SCOPED_FILE_URL STRING, -- URL con scope\n",
        "    CHUNK STRING, -- Fragmento de texto\n",
        "    CHUNK_INDEX INTEGER, -- Índice del fragmento\n",
        "    CATEGORY STRING -- Contendrá la categoría del documento para filtrado\n",
        ");\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Paso 2. Usar la función CORTEX PARSE_DOCUMENT para leer los documentos del área de staging. No es necesario crear embeddings ya que serán gestionados automáticamente por el servicio Cortex Search más adelante.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "sql"
        }
      },
      "outputs": [],
      "source": [
        "-- Insertar los chunks de texto en la tabla\n",
        "INSERT INTO DOCS_CHUNKS_TABLE (relative_path, size, file_url,\n",
        "                            scoped_file_url, chunk, chunk_index)\n",
        "SELECT \n",
        "    relative_path, \n",
        "    size,\n",
        "    file_url, \n",
        "    scoped_file_url,\n",
        "    c.value::TEXT AS chunk,\n",
        "    c.INDEX::INTEGER AS chunk_index\n",
        "FROM \n",
        "    RAW_TEXT,\n",
        "    LATERAL FLATTEN(\n",
        "        input => SNOWFLAKE.CORTEX.SPLIT_TEXT_RECURSIVE_CHARACTER (\n",
        "            EXTRACTED_LAYOUT,\n",
        "            'markdown',\n",
        "            1512,  -- Tamaño máximo del chunk\n",
        "            256,   -- Overlap entre chunks\n",
        "            ['\\n\\n', '\\n', ' ', '']  -- Separadores\n",
        "        )\n",
        "    ) c;\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Etiquetar la categoría del producto\n",
        "\n",
        "Vamos a usar el poder de los Modelos de Lenguaje Grande y la función [CLASSIFY_TEXT](https://docs.snowflake.com/en/sql-reference/functions/classify_text-snowflake-cortex) para clasificar fácilmente los documentos que estamos ingiriendo en nuestra aplicación RAG. Vamos a pasar el nombre del documento y el primer chunk de texto a la función classify_text.\n",
        "\n",
        "Primero crearemos una tabla temporal con cada nombre de archivo único y pasaremos ese nombre de archivo y el primer chunk de texto a CLASSIFY_TEXT. La clasificación no es obligatoria para Cortex Search, pero queremos usarla aquí para también demostrar la búsqueda híbrida.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "sql"
        }
      },
      "outputs": [],
      "source": [
        "-- Crear tabla temporal con las categorías de los documentos\n",
        "CREATE OR REPLACE TEMPORARY TABLE docs_categories AS \n",
        "WITH unique_documents AS (\n",
        "    SELECT\n",
        "        DISTINCT relative_path, \n",
        "        chunk\n",
        "    FROM\n",
        "        docs_chunks_table\n",
        "    WHERE \n",
        "        chunk_index = 0  -- Solo usar el primer chunk para clasificación\n",
        "),\n",
        "docs_category_cte AS (\n",
        "    SELECT\n",
        "        relative_path,\n",
        "        TRIM(\n",
        "            SNOWFLAKE.CORTEX.CLASSIFY_TEXT (\n",
        "                'Título:' || relative_path || ' Contenido:' || chunk, \n",
        "                ['Bicicleta', 'Esquí']  -- Categorías posibles\n",
        "            )['label'], \n",
        "            '\"'\n",
        "        ) AS category\n",
        "    FROM\n",
        "        unique_documents\n",
        ")\n",
        "SELECT\n",
        "    *\n",
        "FROM\n",
        "    docs_category_cte;\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Puedes revisar esa tabla para identificar cuántas categorías se han creado y si son correctas:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "sql"
        }
      },
      "outputs": [],
      "source": [
        "-- Verificar las categorías creadas\n",
        "SELECT category \n",
        "FROM docs_categories \n",
        "GROUP BY category;\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "También podemos verificar que la categoría de cada documento sea correcta:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "sql"
        }
      },
      "outputs": [],
      "source": [
        "-- Ver todas las categorías asignadas\n",
        "SELECT * FROM docs_categories;\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ahora podemos actualizar la tabla con los chunks de texto que serán usados por el servicio Cortex Search para incluir la categoría de cada documento:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "sql"
        }
      },
      "outputs": [],
      "source": [
        "-- Actualizar la tabla principal con las categorías\n",
        "UPDATE docs_chunks_table \n",
        "SET category = docs_categories.category\n",
        "FROM docs_categories\n",
        "WHERE docs_chunks_table.relative_path = docs_categories.relative_path;\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Crear Servicio Cortex Search\n",
        "\n",
        "El siguiente paso es crear el SERVICIO CORTEX SEARCH en la tabla que creamos antes.\n",
        "\n",
        "- El nombre del servicio es **CC_SEARCH_SERVICE_CS**.\n",
        "- El servicio usará la columna **chunk** para crear embeddings y realizar recuperación basada en búsqueda de similitud.\n",
        "- La columna **category** podrá ser usada como filtro.\n",
        "- Para mantener este servicio actualizado, se usará el warehouse **COMPUTE_WH**. NOTA: Puedes reemplazar el nombre del warehouse con otro al que tengas acceso.\n",
        "- El servicio se refrescará cada minuto.\n",
        "- Los datos recuperados contendrán el chunk, relative_path, file_url y category.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "sql"
        }
      },
      "outputs": [],
      "source": [
        "-- Crear el servicio Cortex Search\n",
        "CREATE OR REPLACE CORTEX SEARCH SERVICE CC_SEARCH_SERVICE_CS\n",
        "ON chunk  -- Columna sobre la que se crean embeddings\n",
        "ATTRIBUTES category  -- Columnas que pueden usarse como filtros\n",
        "WAREHOUSE = COMPUTE_WH  -- Warehouse para mantenimiento\n",
        "TARGET_LAG = '1 minute'  -- Frecuencia de actualización\n",
        "AS (\n",
        "    SELECT \n",
        "        chunk,\n",
        "        chunk_index,\n",
        "        relative_path,\n",
        "        file_url,\n",
        "        category\n",
        "    FROM docs_chunks_table\n",
        ");\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Consultar el Servicio Cortex Search\n",
        "\n",
        "Ahora podemos consultar nuestro servicio usando SQL:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "sql"
        }
      },
      "outputs": [],
      "source": [
        "-- Ejemplo de consulta básica al servicio Cortex Search\n",
        "SELECT \n",
        "    chunk,\n",
        "    relative_path,\n",
        "    category\n",
        "FROM TABLE(\n",
        "    CC_SEARCH_SERVICE_CS.SEARCH(\n",
        "        'cuáles son las especificaciones de las bicicletas de carretera?',\n",
        "        10  -- Número de resultados a devolver\n",
        "    )\n",
        ");\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "sql"
        }
      },
      "outputs": [],
      "source": [
        "-- Ejemplo de consulta con filtro por categoría\n",
        "SELECT \n",
        "    chunk,\n",
        "    relative_path,\n",
        "    category\n",
        "FROM TABLE(\n",
        "    CC_SEARCH_SERVICE_CS.SEARCH(\n",
        "        'qué productos de esquí están disponibles?',\n",
        "        10,\n",
        "        {'category': 'Esquí'}  -- Filtrar solo productos de esquí\n",
        "    )\n",
        ");\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Procesamiento Automático de Nuevos Documentos\n",
        "\n",
        "Mantener tu sistema RAG actualizado cuando se agregan, eliminan o actualizan nuevos documentos puede ser tedioso. Snowflake lo hace muy fácil. Por un lado, Cortex Search es un servicio auto-gestionado. Solo necesitamos agregar, eliminar o actualizar filas en la tabla donde se ha habilitado Cortex Search Service y automáticamente el servicio actualizará los índices y creará nuevos embeddings basándose en la frecuencia definida durante la creación del servicio.\n",
        "\n",
        "Además, podemos usar características de Snowflake como Streams, Tasks y Stored Procedures para procesar automáticamente nuevos archivos a medida que se agregan a Snowflake.\n",
        "\n",
        "Primero creamos dos streams para el área de staging DOCS. Uno se usará para procesar eliminaciones y otro para procesar inserciones. Los Streams capturan los cambios en la Directory Table usada para el área de staging DOCS. Así podemos rastrear nuevas actualizaciones y eliminaciones:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "sql"
        }
      },
      "outputs": [],
      "source": [
        "-- Crear streams para capturar cambios en el stage\n",
        "CREATE OR REPLACE STREAM insert_docs_stream ON STAGE docs;\n",
        "CREATE OR REPLACE STREAM delete_docs_stream ON STAGE docs;\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Segundo, vamos a definir un Stored Procedure que procese esos streams para:\n",
        "\n",
        "- Eliminar de la tabla docs_chunk_table el contenido de archivos que han sido eliminados del área de staging, para que ya no sean relevantes\n",
        "- Analizar nuevos documentos que han sido agregados al área de staging usando PARSE_DOCUMENT\n",
        "- Fragmentar el nuevo documento en piezas usando SPLIT_TEXT_RECURSIVE_CHARACTER\n",
        "- Clasificar los nuevos documentos y actualizar la etiqueta (este paso es opcional, solo para mostrar lo que es posible)\n",
        "\n",
        "Crear el Stored Procedure:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "sql"
        }
      },
      "outputs": [],
      "source": [
        "-- Crear stored procedure para procesar inserciones y eliminaciones automáticamente\n",
        "CREATE OR REPLACE PROCEDURE insert_delete_docs_sp()\n",
        "RETURNS STRING\n",
        "LANGUAGE SQL\n",
        "AS\n",
        "$$\n",
        "BEGIN\n",
        "    -- Eliminar documentos que fueron borrados del stage\n",
        "    DELETE FROM docs_chunks_table\n",
        "    USING delete_docs_stream\n",
        "    WHERE docs_chunks_table.RELATIVE_PATH = delete_docs_stream.RELATIVE_PATH\n",
        "    AND delete_docs_stream.METADATA$ACTION = 'DELETE';\n",
        "\n",
        "    -- Procesar nuevos documentos agregados\n",
        "    CREATE OR REPLACE TEMPORARY TABLE RAW_TEXT AS\n",
        "    SELECT \n",
        "        RELATIVE_PATH,\n",
        "        SIZE,\n",
        "        FILE_URL,\n",
        "        BUILD_SCOPED_FILE_URL(@docs, relative_path) AS SCOPED_FILE_URL,\n",
        "        TO_VARCHAR (\n",
        "            SNOWFLAKE.CORTEX.PARSE_DOCUMENT (\n",
        "                '@docs',\n",
        "                RELATIVE_PATH,\n",
        "                {'mode': 'LAYOUT'}\n",
        "            ):content\n",
        "        ) AS EXTRACTED_LAYOUT \n",
        "    FROM \n",
        "        insert_docs_stream\n",
        "    WHERE \n",
        "        METADATA$ACTION = 'INSERT';\n",
        "\n",
        "    -- Insertar nuevos chunks de documentos\n",
        "    INSERT INTO docs_chunks_table (relative_path, size, file_url,\n",
        "                            scoped_file_url, chunk, chunk_index)\n",
        "    SELECT \n",
        "        relative_path, \n",
        "        size,\n",
        "        file_url, \n",
        "        scoped_file_url,\n",
        "        c.value::TEXT AS chunk,\n",
        "        c.INDEX::INTEGER AS chunk_index\n",
        "    FROM \n",
        "        RAW_TEXT,\n",
        "        LATERAL FLATTEN(\n",
        "            input => SNOWFLAKE.CORTEX.SPLIT_TEXT_RECURSIVE_CHARACTER (\n",
        "                EXTRACTED_LAYOUT,\n",
        "                'markdown',\n",
        "                1512,\n",
        "                256,\n",
        "                ['\\n\\n', '\\n', ' ', '']\n",
        "            )\n",
        "        ) c;\n",
        "\n",
        "    -- Clasificar los nuevos documentos\n",
        "    CREATE OR REPLACE TEMPORARY TABLE docs_categories AS \n",
        "    WITH unique_documents AS (\n",
        "        SELECT DISTINCT\n",
        "            d.relative_path, \n",
        "            d.chunk\n",
        "        FROM\n",
        "            docs_chunks_table d\n",
        "        INNER JOIN\n",
        "            RAW_TEXT r\n",
        "        ON d.relative_path = r.relative_path\n",
        "        WHERE \n",
        "            d.chunk_index = 0\n",
        "    ),\n",
        "    docs_category_cte AS (\n",
        "        SELECT\n",
        "            relative_path,\n",
        "            TRIM(\n",
        "                SNOWFLAKE.CORTEX.CLASSIFY_TEXT (\n",
        "                    'Título:' || relative_path || ' Contenido:' || chunk, \n",
        "                    ['Bicicleta', 'Esquí']\n",
        "                )['label'], \n",
        "                '\"'\n",
        "            ) AS category\n",
        "        FROM\n",
        "            unique_documents\n",
        "    )\n",
        "    SELECT\n",
        "        *\n",
        "    FROM\n",
        "        docs_category_cte;\n",
        "\n",
        "    -- Actualizar categorías\n",
        "    UPDATE docs_chunks_table \n",
        "    SET category = docs_categories.category\n",
        "    FROM docs_categories\n",
        "    WHERE docs_chunks_table.relative_path = docs_categories.relative_path;\n",
        "    \n",
        "    RETURN 'Procesamiento completado con éxito';\n",
        "END;\n",
        "$$;\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ahora podemos crear una Task que cada X minutos puede verificar si hay nuevos datos en el stream y tomar una acción. Estamos configurando el schedule a 5 minutos para que puedas seguir la ejecución, pero siéntete libre de reducir el tiempo a 1 minuto si es necesario. Considera qué sería mejor para tu aplicación y con qué frecuencia se actualizan los nuevos documentos.\n",
        "\n",
        "Definimos:\n",
        "- **Dónde**: Esto se ejecutará usando el warehouse **COMPUTE_WH**. Por favor cambia al nombre de tu propio Warehouse.\n",
        "- **Cuándo**: Verificar cada 5 minutos, y ejecutar en caso de que haya nuevos registros en el stream delete_docs_stream (también podríamos usar el otro stream)\n",
        "- **Qué hacer**: llamar al stored procedure insert_delete_docs_sp()\n",
        "\n",
        "Ejecuta este código en tu worksheet de Snowflake para crear la task:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "sql"
        }
      },
      "outputs": [],
      "source": [
        "-- Crear task para ejecutar el stored procedure automáticamente\n",
        "CREATE OR REPLACE TASK insert_delete_docs_task\n",
        "    WAREHOUSE = COMPUTE_WH\n",
        "    SCHEDULE = '5 minute'  -- Ejecutar cada 5 minutos\n",
        "    WHEN SYSTEM$STREAM_HAS_DATA('delete_docs_stream')  -- Solo cuando hay cambios\n",
        "AS\n",
        "    CALL insert_delete_docs_sp();\n",
        "\n",
        "-- Activar la task\n",
        "ALTER TASK insert_delete_docs_task RESUME;\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
