{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construir un Asistente LLM basado en RAG usando Streamlit y Snowflake Cortex Search\n",
    "\n",
    "*NOTA: Para requisitos previos y otras instrucciones, consulta la [Guía de Inicio Rápido](https://quickstarts.snowflake.com/guide/ask_questions_to_your_own_documents_with_snowflake_cortex_search/index.html#0).*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuración Inicial\n",
    "\n",
    "Crear una base de datos y un schema.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "--CREATE DATABASE IF NOT EXISTS CC_QUICKSTART_CORTEX_SEARCH_DOCS;\n",
    "--CREATE SCHEMA IF NOT EXISTS DATA;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Organizar Documentos y Crear Función de Pre-procesamiento\n",
    "\n",
    "Paso 1. Descargar [documentos PDF de muestra](https://github.com/sfc-gh-jparrado/HOL/tree/main/Cortex_Search/documentos).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paso 2. Crear un Stage con Directory Table donde subirás tus documentos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "create or replace stage docs ENCRYPTION = (TYPE = 'SNOWFLAKE_SSE') DIRECTORY = ( ENABLE = true );\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paso 3. Subir documentos a tu área de staging\n",
    "\n",
    "- Selecciona Data en el menú izquierdo\n",
    "- Haz clic en tu base de datos CC_QUICKSTART_CORTEX_SEARCH_DOCS\n",
    "- Haz clic en tu schema DATA\n",
    "- Haz clic en Stages y selecciona DOCS\n",
    "- En la esquina superior derecha haz clic en el botón **+Files**\n",
    "- Arrastra y suelta los documentos PDF que descargaste\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paso 4. Verificar que los archivos se han subido correctamente\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "ls @docs;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-procesar y Etiquetar Documentos\n",
    "\n",
    "Paso 1. Crear la tabla donde vamos a almacenar los chunks para cada PDF.\n",
    "\n",
    "Vamos a aprovechar las funciones nativas de procesamiento de documentos de Snowflake para preparar los documentos antes de habilitar Cortex Search. También vamos a usar la función Cortex CLASSIFY_TEXT para etiquetar el tipo de documento que se está procesando, de modo que podamos usar esos metadatos para filtrar búsquedas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE TEMPORARY TABLE RAW_TEXT AS\n",
    "SELECT \n",
    "    RELATIVE_PATH,\n",
    "    SIZE,\n",
    "    FILE_URL,\n",
    "    build_scoped_file_url(@docs, relative_path) as scoped_file_url,\n",
    "    TO_VARCHAR (\n",
    "        SNOWFLAKE.CORTEX.PARSE_DOCUMENT (\n",
    "            '@docs',\n",
    "            RELATIVE_PATH,\n",
    "            {'mode': 'LAYOUT'} ):content\n",
    "        ) AS EXTRACTED_LAYOUT \n",
    "FROM \n",
    "    DIRECTORY('@docs');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "create or replace TABLE DOCS_CHUNKS_TABLE ( \n",
    "    RELATIVE_PATH VARCHAR(16777216), -- Ruta relativa al archivo PDF\n",
    "    SIZE NUMBER(38,0), -- Tamaño del PDF\n",
    "    FILE_URL VARCHAR(16777216), -- URL para el PDF\n",
    "    SCOPED_FILE_URL VARCHAR(16777216), -- URL con scope (puedes elegir cuál mantener según tu caso de uso)\n",
    "    CHUNK VARCHAR(16777216), -- Fragmento de texto\n",
    "    CHUNK_INDEX INTEGER, -- Índice del texto\n",
    "    CATEGORY VARCHAR(16777216) -- Contendrá la categoría del documento para habilitar filtrado\n",
    ");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paso 2. Usar la función CORTEX PARSE_DOCUMENT para leer los documentos PDF del área de staging. No es necesario crear embeddings ya que serán gestionados automáticamente por el servicio Cortex Search más adelante.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    " insert into docs_chunks_table (relative_path, size, file_url,\n",
    "                            scoped_file_url, chunk, chunk_index)\n",
    "\n",
    "    select relative_path, \n",
    "            size,\n",
    "            file_url, \n",
    "            scoped_file_url,\n",
    "            c.value::TEXT as chunk,\n",
    "            c.INDEX::INTEGER as chunk_index\n",
    "            \n",
    "    from \n",
    "        raw_text,\n",
    "        LATERAL FLATTEN( input => SNOWFLAKE.CORTEX.SPLIT_TEXT_RECURSIVE_CHARACTER (\n",
    "              EXTRACTED_LAYOUT,\n",
    "              'markdown',\n",
    "              1512,\n",
    "              256,\n",
    "              ['\\n\\n', '\\n', ' ', '']\n",
    "           )) c;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Etiquetar la categoría del producto\n",
    "\n",
    "Vamos a usar el poder de los Modelos de Lenguaje Grande y la función [CLASSIFY_TEXT](https://docs.snowflake.com/en/sql-reference/functions/classify_text-snowflake-cortex) para clasificar fácilmente los documentos que estamos ingiriendo en nuestra aplicación RAG. Vamos a pasar el nombre del documento y el primer chunk de texto a la función classify_text.\n",
    "\n",
    "Primero crearemos una tabla temporal con cada nombre de archivo único y pasaremos ese nombre de archivo y el primer chunk de texto a CLASSIFY_TEXT. La clasificación no es obligatoria para Cortex Search, pero queremos usarla aquí para también demostrar la búsqueda híbrida.\n",
    "\n",
    "Ejecuta este SQL para crear esa tabla:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE TEMPORARY TABLE docs_categories AS WITH unique_documents AS (\n",
    "  SELECT\n",
    "    DISTINCT relative_path, chunk\n",
    "  FROM\n",
    "    docs_chunks_table\n",
    "  WHERE \n",
    "    chunk_index = 0\n",
    "  ),\n",
    " docs_category_cte AS (\n",
    "  SELECT\n",
    "    relative_path,\n",
    "    TRIM(snowflake.cortex.CLASSIFY_TEXT (\n",
    "      'Title:' || relative_path || 'Content:' || chunk, ['Bike', 'Snow']\n",
    "     )['label'], '\"') AS category\n",
    "  FROM\n",
    "    unique_documents\n",
    ")\n",
    "SELECT\n",
    "  *\n",
    "FROM\n",
    "  docs_category_cte;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puedes revisar esa tabla para identificar cuántas categorías se han creado y si son correctas:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "select category from docs_categories group by category;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También podemos verificar que la categoría de cada documento sea correcta:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "select * from docs_categories;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora podemos actualizar la tabla con los chunks de texto que serán usados por el servicio Cortex Search para incluir la categoría de cada documento:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "update docs_chunks_table \n",
    "  SET category = docs_categories.category\n",
    "  from docs_categories\n",
    "  where  docs_chunks_table.relative_path = docs_categories.relative_path;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crear Servicio Cortex Search\n",
    "\n",
    "El siguiente paso es crear el SERVICIO CORTEX SEARCH en la tabla que creamos antes.\n",
    "\n",
    "- El nombre del servicio es CC_SEARCH_SERVICE_CS.\n",
    "- El servicio usará la columna chunk para crear embeddings y realizar recuperación basada en búsqueda de similitud.\n",
    "- La columna category podrá ser usada como filtro.\n",
    "- Para mantener este servicio actualizado, se usará el warehouse COMPUTE_WH. NOTA: Puedes reemplazar el nombre del warehouse con otro al que tengas acceso.\n",
    "- El servicio se refrescará cada minuto.\n",
    "- Los datos recuperados contendrán el chunk, relative_path, file_url y category.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "create or replace CORTEX SEARCH SERVICE CC_SEARCH_SERVICE_CS\n",
    "ON chunk\n",
    "ATTRIBUTES category\n",
    "warehouse = COMPUTE_WH\n",
    "TARGET_LAG = '1 minute'\n",
    "as (\n",
    "    select chunk,\n",
    "        chunk_index,\n",
    "        relative_path,\n",
    "        file_url,\n",
    "        category\n",
    "    from docs_chunks_table\n",
    ");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construir Interfaz de Chat\n",
    "\n",
    "Para construir y ejecutar la interfaz de chat en Streamlit, continúa y completa los pasos descritos en la [Guía de Inicio Rápido](https://quickstarts.snowflake.com/guide/ask_questions_to_your_own_documents_with_snowflake_cortex_search/index.html#4).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procesamiento Automático de Nuevos Documentos\n",
    "Mantener tu sistema RAG actualizado cuando se agregan, eliminan o actualizan nuevos documentos puede ser tedioso. Snowflake lo hace muy fácil. Por un lado, Cortex Search es un servicio auto-gestionado. Solo necesitamos agregar, eliminar o actualizar filas en la tabla donde se ha habilitado el servicio Cortex Search y automáticamente el servicio actualizará los índices y creará nuevos embeddings basándose en la frecuencia definida durante la creación del servicio.\n",
    "\n",
    "Además, podemos usar características de Snowflake como Streams, Tasks y Stored Procedures para procesar automáticamente nuevos archivos PDF a medida que se agregan a Snowflake. \n",
    "\n",
    "Primero creamos dos streams para el área de staging DOCS. Uno se usará para procesar eliminaciones y otro para procesar inserciones. Los Streams capturan los cambios en la Directory Table usada para el área de staging DOCS. Así podemos rastrear nuevas actualizaciones y eliminaciones:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "create or replace stream insert_docs_stream on stage docs;\n",
    "create or replace stream delete_docs_stream on stage docs;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Segundo, vamos a definir un Stored Procedure que procese esos streams para:\n",
    "\n",
    "- Eliminar de la tabla docs_chunk_table el contenido de archivos que han sido eliminados del área de staging, para que ya no sean relevantes\n",
    "- Analizar nuevos documentos PDF que han sido agregados al área de staging usando PARSE_DOCUMENT\n",
    "- Fragmentar el nuevo documento en piezas usando SPLIT_TEXT_RECURSIVE_CHARACTER\n",
    "- Clasificar los nuevos documentos y actualizar la etiqueta (este paso es opcional, solo para mostrar lo que es posible)\n",
    "\n",
    "Crear el Stored Procedure:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "create or replace procedure insert_delete_docs_sp()\n",
    "RETURNS VARCHAR\n",
    "LANGUAGE SQL\n",
    "AS\n",
    "$$\n",
    "BEGIN\n",
    "\n",
    "DELETE FROM docs_chunks_table\n",
    "    USING delete_docs_stream\n",
    "    WHERE docs_chunks_table.RELATIVE_PATH = delete_docs_stream.RELATIVE_PATH\n",
    "    and delete_docs_stream.METADATA$ACTION = 'DELETE';\n",
    "\n",
    "\n",
    "CREATE or replace TEMPORARY table RAW_TEXT AS\n",
    "    SELECT \n",
    "        RELATIVE_PATH,\n",
    "        SIZE,\n",
    "        FILE_URL,\n",
    "        build_scoped_file_url(@docs, relative_path) as scoped_file_url,\n",
    "        TO_VARCHAR (\n",
    "            SNOWFLAKE.CORTEX.PARSE_DOCUMENT (\n",
    "                '@docs',\n",
    "                RELATIVE_PATH,\n",
    "                {'mode': 'LAYOUT'} ):content\n",
    "            ) AS EXTRACTED_LAYOUT \n",
    "    FROM \n",
    "        insert_docs_stream\n",
    "    WHERE \n",
    "        METADATA$ACTION = 'INSERT';\n",
    "\n",
    "    -- Insert new docs chunks\n",
    "    insert into docs_chunks_table (relative_path, size, file_url,\n",
    "                            scoped_file_url, chunk, chunk_index)\n",
    "\n",
    "    select relative_path, \n",
    "            size,\n",
    "            file_url, \n",
    "            scoped_file_url,\n",
    "            c.value::TEXT as chunk,\n",
    "            c.INDEX::INTEGER as chunk_index\n",
    "            \n",
    "    from \n",
    "        RAW_TEXT,\n",
    "        LATERAL FLATTEN( input => SNOWFLAKE.CORTEX.SPLIT_TEXT_RECURSIVE_CHARACTER (\n",
    "              EXTRACTED_LAYOUT,\n",
    "              'markdown',\n",
    "              1512,\n",
    "              256,\n",
    "              ['\\n\\n', '\\n', ' ', '']\n",
    "           )) c;\n",
    "\n",
    "    -- Classify the new documents\n",
    "\n",
    "    CREATE OR REPLACE TEMPORARY TABLE docs_categories AS \n",
    "    WITH unique_documents AS (\n",
    "      SELECT DISTINCT\n",
    "        d.relative_path, d.chunk\n",
    "      FROM\n",
    "        docs_chunks_table d\n",
    "      INNER JOIN\n",
    "        RAW_TEXT r\n",
    "        ON d.relative_path = r.relative_path\n",
    "      WHERE \n",
    "        d.chunk_index = 0\n",
    "    ),\n",
    "    docs_category_cte AS (\n",
    "      SELECT\n",
    "        relative_path,\n",
    "        TRIM(snowflake.cortex.CLASSIFY_TEXT (\n",
    "          'Title:' || relative_path || 'Content:' || chunk, ['Bike', 'Snow']\n",
    "        )['label'], '\"') AS category\n",
    "      FROM\n",
    "        unique_documents\n",
    "    )\n",
    "    SELECT\n",
    "      *\n",
    "    FROM\n",
    "      docs_category_cte;\n",
    "\n",
    "    -- Update cathegories\n",
    "\n",
    "    update docs_chunks_table \n",
    "        SET category = docs_categories.category\n",
    "        from docs_categories\n",
    "        where  docs_chunks_table.relative_path = docs_categories.relative_path;\n",
    "\n",
    "END;\n",
    "$$;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora podemos crear una Task que cada X minutos puede verificar si hay nuevos datos en el stream y tomar una acción. Estamos configurando el schedule a 5 minutos para que puedas seguir la ejecución, pero siéntete libre de reducir el tiempo a 1 minuto si es necesario. Considera qué sería mejor para tu aplicación y con qué frecuencia se actualizan los nuevos documentos.\n",
    "\n",
    "Definimos:\n",
    "  - Dónde: Esto se ejecutará usando el warehouse **COMPUTE_WH**. Por favor cambia al nombre de tu propio Warehouse.\n",
    "  - Cuándo: Verificar cada 5 minutos, y ejecutar en caso de que haya nuevos registros en el stream delete_docs_stream (también podríamos usar el otro stream)\n",
    "  - Qué hacer: llamar al stored procedure insert_delete_docs_sp()\n",
    "\n",
    "Ejecuta este código en tu worksheet de Snowflake para crear la task:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "create or replace task insert_delete_docs_task\n",
    "    warehouse = COMPUTE_WH\n",
    "    schedule = '5 minute'\n",
    "    when system$stream_has_data('delete_docs_stream')\n",
    "as\n",
    "    call insert_delete_docs_sp();\n",
    "\n",
    "\n",
    "alter task  insert_delete_docs_task resume;\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
