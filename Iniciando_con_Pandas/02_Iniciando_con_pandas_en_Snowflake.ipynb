{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "lastEditStatus": {
   "notebookId": "vyrg7ewzauybx5c3b3eh",
   "authorId": "346180102309",
   "authorName": "JPARRADO",
   "authorEmail": "jorge.parrado@snowflake.com",
   "sessionId": "925db47c-61da-49c5-a8d5-b4b634c0fe5f",
   "lastEditTime": 1746602180739
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69bc8786-3ea7-4b21-a53f-09675d86534b",
   "metadata": {
    "collapsed": false,
    "name": "cell1"
   },
   "source": "# Primeros pasos con pandas en Snowflake\n\n[pandas on Snowflake](https://docs.snowflake.com/developer-guide/snowpark/python/snowpark-pandas) permite a los desarrolladores ejecutar su código de pandas directamente en sus datos en Snowflake. Los usuarios podrán obtener la misma experiencia nativa de pandas que conocen y aman con el rendimiento, la escala y la gobernanza de Snowflake.\n\nEn esta guía de inicio rápido, mostraremos cómo puede comenzar a ejecutar pandas en Snowflake a través de la API de pandas de Snowpark. También veremos que la API de pandas de Snowpark es muy similar a la API nativa de pandas y le permite escalar sus canalizaciones tradicionales de pandas con solo unas pocas líneas de cambio. Puede ejecutar este cuaderno en un cuaderno de Snowflake.\n\n## Uso de la API de pandas de Snowpark\n\nLa API de pandas de Snowpark está disponible como parte del paquete Snowpark Python (versión 1.17 y superior). Snowpark Python viene preinstalado con el entorno de Snowflake Notebooks. Además, deberá agregar el paquete `modin` en el menú desplegable `Packages`.\n\n- Para instalar Modin, seleccione `modin` de `Packages` y asegúrese de que la versión sea `0.30.1`."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b806a16b-b666-4e38-b11c-5db618772a12",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "cell2"
   },
   "outputs": [],
   "source": [
    "# Import the Snowpark pandas plugin for modin\n",
    "import snowflake.snowpark.modin.plugin\n",
    "import modin.pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7b7455-c1ad-4ad8-bf85-1ed2b0d516b8",
   "metadata": {
    "collapsed": false,
    "name": "cell3"
   },
   "source": "## Crear sesión de Snowpark\nSnowpark pandas requiere un objeto `Session` activo para conectarse a sus datos en Snowflake. En la siguiente celda, inicializaremos un objeto Session e importaremos Snowpark pandas como `pd`. Asegúrese de utilizar una base de datos en la que tenga permisos de escritura al crear la sesión, ya que Snowpark pandas requiere permisos de escritura."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2342c8-f661-4f86-8245-813f8b7ad0ab",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "cell4"
   },
   "outputs": [],
   "source": [
    "# Access current Snowpark session\n",
    "from snowflake.snowpark.context import get_active_session\n",
    "session = get_active_session()\n",
    "# Add a query tag to the session for troubleshooting and monitoring\n",
    "session.query_tag = {\"origin\":\"sf_sit-is\", \n",
    "                     \"name\":\"pandas_on_snowflake\", \n",
    "                     \"version\":{\"major\":1, \"minor\":0},\n",
    "                     \"attributes\":{\"is_quickstart\":1, \"source\":\"notebook\", \"vignette\":\"snowpark_pandas\"}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3812257-0a82-43a0-aaac-d00681558890",
   "metadata": {
    "collapsed": false,
    "name": "cell5"
   },
   "source": "## Lectura de datos desde Snowflake\nHoy, analizaremos los datos de series de tiempo del [conjunto de datos de Finanzas y Economía de Cybersyn](https://app.snowflake.com/marketplace/listing/GZTSZAS2KF7/cybersyn-inc-financial-economic-essentials). Puede encontrar las instrucciones para configurar el conjunto de datos para este tutorial [aquí](https://quickstarts.snowflake.com/guide/getting_started_with_pandas_on_snowflake/#1).\n\n¡Comencemos leyendo la tabla `stock_price_timeseries` en un DataFrame!\n\nVerifique que tengas permisos de escritura en la base de datos con la que inicializó la `Session` de Snowpark. Si está leyendo de la tabla `stock_price_timeseries`, su `Session` debe configurarse para usar una base de datos diferente en la que tenga permisos de escritura. La celda a continuación utiliza el nombre completo de la tabla para garantizar que la lectura se realice correctamente, incluso si la `Session` está configurada para usar una base de datos diferente."
  },
  {
   "cell_type": "code",
   "id": "1527de2e-e722-4b80-a24f-7e971f540f73",
   "metadata": {
    "language": "sql",
    "name": "cell35"
   },
   "outputs": [],
   "source": "-- Cuantos registros tiene la tabla?\nselect count(1) from PANDAS_DB.PUBLIC.STOCK_PRICE_TIMESERIES;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03298234-aabe-4548-99b1-bfdb609bdafb",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "cell6"
   },
   "outputs": [],
   "source": "# Read data into a Snowpark pandas df \nfrom time import perf_counter\nstart = perf_counter()\nspd_df = pd.read_snowflake(\"PANDAS_DB.PUBLIC.STOCK_PRICE_TIMESERIES\")\nend = perf_counter()\ndata_size = len(spd_df)\nprint(f\"Snowpark pandas tardó {round(end - start,3)} segundos en leer una tabla con {data_size:,} registros!\")\nsnow_time = end - start"
  },
  {
   "cell_type": "markdown",
   "id": "28f231df-fcc4-41e1-8809-314bae4b38ff",
   "metadata": {
    "collapsed": false,
    "name": "cell7"
   },
   "source": "Ahora hagamos lo mismo leyendo los datos en pandas nativo. Hay dos enfoques comunes para hacer esto:\n\n1) Crear un [DataFrame de Snowpark](https://docs.snowflake.com/en/developer-guide/snowpark/python/working-with-dataframes#return-the-contents-of-a-dataframe-as-a-pandas-dataframe) y llamar a [`to_pandas`](https://docs.snowflake.com/developer-guide/snowpark/reference/python/latest/snowpark/api/snowflake.snowpark.DataFrame.to_pandas) para exportar los resultados a un DataFrame de pandas.\n```python\nsnowpark_df = session.table(\"STOCK_PRICE_TIMESERIES\")\nnative_pd_df = snowpark_df.to_pandas()\n```\n\n2) Usemos el [Snowflake Connector for Python](https://docs.snowflake.com/en/developer-guide/python-connector/python-connector-pandas) para consultar y exportar los resultados desde Snowflake a un Dataframe de Pandas usando [`fetch_pandas_all`](https://docs.snowflake.com/en/developer-guide/python-connector/python-connector-api#fetch_pandas_all)\n\n```python\n# Crear un objeto cursor\ncur = session.connection.cursor()\n# Ejecutar una sentencia que generará un conjunto de resultados\ncur.execute(\"select * from STOCK_PRICE_TIMESERIES\")\n# Obtener todas las filas de un cursor y cargarlas en un DataFrame de pandas\nnative_pd_df = cur.fetch_pandas_all()\n```\n\nUtilizaremos el segundo enfoque a continuación y mediremos el tiempo que tardan estas operaciones. (Nota: ¡Esto puede tardar varios minutos!)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69ac2fd-c636-4bb5-a27d-58a5e4cbea7e",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "cell8",
    "scrolled": true
   },
   "outputs": [],
   "source": "start = perf_counter()\ncur = session.connection.cursor()\ncur.execute(\"select * from PANDAS_DB.PUBLIC.STOCK_PRICE_TIMESERIES\")\nnative_pd_df = cur.fetch_pandas_all()\nend = perf_counter()\nprint(f\"Pandas nativo tardó {round(end - start,3)} segundos en leer la misma tabla con {data_size:,} registros!!\")"
  },
  {
   "cell_type": "markdown",
   "id": "72085630-11c4-4df1-9627-7c50c0957906",
   "metadata": {
    "collapsed": false,
    "name": "cell9"
   },
   "source": "Como puede ver, lleva mucho más tiempo exportar la tabla de Snowflake a la memoria para operar con pandas nativo que para que Snowpark pandas lea la tabla directamente. Esto también puede provocar que la sesión del cuaderno se bloquee si los datos exportados exceden lo que cabe en la memoria."
  },
  {
   "cell_type": "markdown",
   "id": "62c3dee5-62fe-46c6-a216-133a0140222d",
   "metadata": {
    "collapsed": false,
    "name": "cell10"
   },
   "source": "## Examina los Datos Crudos\nEchemos un vistazo a los datos con los que vamos a trabajar. Inspeccionaremos las primeras cinco filas del dataframe y las imprimiremos utilizando la visualización interactiva de dataframes de Streamlit (`st.dataframe`)."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a623bed-aed9-4cdb-a3c8-33e9e7da52af",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "cell11"
   },
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "st.dataframe(spd_df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2822e6ae-9810-4ca1-8646-660eb3e68d97",
   "metadata": {
    "collapsed": false,
    "name": "cell12"
   },
   "source": "## Filtrando los Datos\nEchemos un vistazo a algunas transformaciones de datos comunes, ¡empezando por el filtrado! ¡Filtremos las acciones que cotizan en la Bolsa de Nueva York!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4218fceb-68f1-41be-8c08-3f6ad51424d5",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "cell13"
   },
   "outputs": [],
   "source": "start = perf_counter()\nnyse_spd_df = spd_df[(spd_df['PRIMARY_EXCHANGE_CODE'] == 'NYS')]\nrepr(nyse_spd_df)\nend = perf_counter()\nst.dataframe(nyse_spd_df.head())\nprint(f\"Filtrando por acciones pertenecientes a la NYSE tardó {round(end - start,3)} segundos en Snowpark pandas\")"
  },
  {
   "cell_type": "markdown",
   "id": "c2e325b5-8e24-4dbc-93da-c2e93d84590f",
   "metadata": {
    "collapsed": false,
    "name": "cell14"
   },
   "source": "## Filtrado Granular de Datos\nIntentemos un filtro aún más granular: ¡filtremos para la apertura pre-mercado de las acciones que tienen los siguientes tickers:\n* GOOG (Alphabet, Inc.)\n* MSFT (Microsoft)\n* SNOW (Snowflake)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d456c29-7689-4599-bcd6-02c646ef8f58",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "cell15"
   },
   "outputs": [],
   "source": "start = perf_counter()\nfiltered_spd_df = spd_df[((spd_df['TICKER'] == 'GOOG') | (spd_df['TICKER'] == 'MSFT') | (spd_df['TICKER'] == 'SNOW')) & (spd_df['VARIABLE_NAME'] == 'Pre-Market Open')]\nrepr(filtered_spd_df)\nend = perf_counter()\nst.dataframe(filtered_spd_df.head())\nprint(f\"Filtrar por el precio de apertura pre-mercado para las acciones mencionadas anteriormente tardó {round(end - start,3)} segundos en Snowpark pandas\")"
  },
  {
   "cell_type": "markdown",
   "id": "a8d5cb07-ccce-481e-b41e-770d4de91b0f",
   "metadata": {
    "collapsed": false,
    "name": "cell16"
   },
   "source": "# Transformando los Datos\nDigamos que quisiéramos analizar el rendimiento de varios precios de acciones a lo largo del tiempo; en ese caso, podría ser más útil tener los valores como columnas, y el nombre del ticker y la fecha como índice, en lugar de la codificación actual. ¡Podemos lograr esto utilizando la API `pivot_table`!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8f893a-c7dc-4e08-bace-3c93ada282cf",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "cell17"
   },
   "outputs": [],
   "source": "start = perf_counter()\nreshape_df = spd_df.pivot_table(index=[\"TICKER\", \"DATE\"], columns=\"VARIABLE_NAME\", values=\"VALUE\")\nrepr(reshape_df)\nend = perf_counter()\nprint(f\"Pivotar el DataFrame tardó {round(end - start,3)} segundos en Snowpark pandas\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c0b9d1-a3be-4d05-9481-f54628f3b793",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "cell18"
   },
   "outputs": [],
   "source": [
    "st.dataframe(reshape_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3121c325-d7cf-47d6-a2d7-e759ece59d11",
   "metadata": {
    "collapsed": false,
    "name": "cell19"
   },
   "source": "## Transformando los Datos\nAhora que hemos reformateado los datos, podemos comenzar a aplicar algunas transformaciones. Empecemos por echar un vistazo a la columna de Mínimo Diario para los tickers mencionados anteriormente. ¡Podemos remuestrear los datos para observar el Mínimo Trimestral para el ticker `GOOG`!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b06f23b-12dc-4387-bb87-bc4cbcff6a85",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "cell20"
   },
   "outputs": [],
   "source": "start = perf_counter()\nresampled_spd_df_all_quarter_low = reshape_df[\"All-Day Low\"][\"GOOG\"].resample(\"91D\").min()\nrepr(resampled_spd_df_all_quarter_low)\nend = perf_counter()\nprint(f\"Remuestrear el DataFrame tardó {round(end - start,3)} segundos en Snowpark pandas\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8978f55a-c28a-4b7f-9f20-4a2952d2a857",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "cell21"
   },
   "outputs": [],
   "source": [
    "print(resampled_spd_df_all_quarter_low)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c512e74d-7316-44de-a644-917270d38fac",
   "metadata": {
    "collapsed": false,
    "name": "cell22"
   },
   "source": "Incluso podemos observar la fluctuación de precios trimestre a trimestre utilizando la API `diff`!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb467dd6-cc74-423f-b17b-46541f5bbff8",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "cell23"
   },
   "outputs": [],
   "source": "start = perf_counter()\nq_o_q_resampled_spd_df_all_quarter_low = resampled_spd_df_all_quarter_low.diff()\nrepr(q_o_q_resampled_spd_df_all_quarter_low)\nend = perf_counter()\nprint(f\"Calcular las diferencias de los datos remuestreados tardó {round(end - start,3)} segundos en Snowpark pandas\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866628d5-5bf9-4212-bba2-bf5e816a70e1",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "cell24"
   },
   "outputs": [],
   "source": [
    "print(q_o_q_resampled_spd_df_all_quarter_low)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7593697-feb5-40a7-9d6c-7c011ad35186",
   "metadata": {
    "collapsed": false,
    "name": "cell25"
   },
   "source": "## Aplicar una función a lo largo de un eje\nAhora queremos aplicar la raíz cuadrada del valor absoluto a cada valor de la serie.\n\nSnowpark pandas soporta `apply`, que aplica alguna función arbitraria de Python definida por el usuario a lo largo de un eje particular del DataFrame o Serie.\n\nLa función de Python se serializa en bytecode de Python y se ejecuta como una UDF dentro del entorno de ejecución del sandbox seguro de Python de Snowpark. El entorno de ejecución de Python de Snowpark está perfectamente integrado con el administrador de paquetes Anaconda, de modo que los usuarios pueden aprovechar sus paquetes de terceros favoritos, como NumPy, para una transformación de datos flexible dentro de su `dataframe.apply`.\n\n**Consejo de Experto:** Si bien llamar a `apply` es conveniente, dado que la implementación subyacente son UDF o UDTF, puede que no esté tan optimizado como las consultas SQL transpiled de otras consultas de Snowpark pandas. Si la función aplicada tiene una operación de dataframe o serie equivalente, recomendamos utilizar esas operaciones en su lugar. Por ejemplo, en lugar de `df.groupby('col1').apply('sum')`, llama directamente a `df.groupby('col1').sum()`."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3775af-8fe8-48f3-abd4-f783c0d27528",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "cell26"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "resampled_all_quarter_low_df_sqrt = q_o_q_resampled_spd_df_all_quarter_low.apply(\n",
    "    lambda x: np.sqrt(abs(x))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45dbe2f-8bd3-46db-8c31-f39994db6b99",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "cell27"
   },
   "outputs": [],
   "source": [
    "resampled_all_quarter_low_df_sqrt = resampled_all_quarter_low_df_sqrt.dropna()\n",
    "print(resampled_all_quarter_low_df_sqrt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76af70b-c0f1-4884-9569-7187b3a16ff3",
   "metadata": {
    "collapsed": false,
    "name": "cell28"
   },
   "source": "## Visualizando tus resultados con Altair\n\npandas se utiliza a menudo en conjunto con bibliotecas de visualización y aprendizaje automático de terceros. Aquí queremos trazar la fluctuación de precios trimestre a trimestre como un gráfico de barras.\n\nPrimero, limpiemos los datos para fines de trazado."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20345b5d-ef02-4641-9ed0-8cea05c5a6dd",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "cell29"
   },
   "outputs": [],
   "source": [
    "# Convert series to dataframe by resetting index\n",
    "plot_df = q_o_q_resampled_spd_df_all_quarter_low.reset_index()\n",
    "# Rename columns\n",
    "plot_df.columns = [\"DATE\", \"QLOW_DIFF\"]\n",
    "# Filter out extreme values\n",
    "plot_df = plot_df[plot_df[\"QLOW_DIFF\"]>-700]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8fc97f-c6fa-4393-a67b-bc751077dc00",
   "metadata": {
    "collapsed": false,
    "name": "cell30"
   },
   "source": "Al llamar a APIs de bibliotecas de terceros con un dataframe de Snowpark pandas, recomendamos convertir el dataframe de Snowpark pandas a un dataframe de pandas llamando a [`to_pandas`](https://docs.snowflake.com/en/developer-guide/snowpark/reference/python/1.21.0/modin/pandas_api/snowflake.snowpark.modin.pandas.to_pandas) antes de pasar el dataframe a la llamada de la biblioteca de terceros.\n\nTen en cuenta que llamar a `to_pandas` extrae tus datos de Snowflake y los carga en la memoria, así que procede con precaución para conjuntos de datos grandes y casos de uso sensibles. Generalmente recomendamos agregar o resumir y exportar solo los datos que utilizarás para trazar utilizando `to_pandas`.\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3d6647-86a4-4f32-9591-e8dfd4383c43",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "cell31"
   },
   "outputs": [],
   "source": "print(\"Tipo de DataFrame de entrada: \", type(plot_df))\npandas_plot_df = plot_df.to_pandas()\nprint(\"Después de to_pandas, tipo de DataFrame de salida: \", type(pandas_plot_df))"
  },
  {
   "cell_type": "markdown",
   "id": "bd12f7cb-949f-462f-8806-79fe6b9734bb",
   "metadata": {
    "collapsed": false,
    "name": "cell32"
   },
   "source": "Ahora podemos usar cualquier biblioteca de visualización de Python, como Altair, para trazar el dataframe de pandas resultante."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb3a7d0-5a66-42a1-a887-b1748bf756fd",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "cell33"
   },
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "alt.Chart(pandas_plot_df).mark_bar(width=10).encode(\n",
    "    x = alt.X(\"DATE:T\"),\n",
    "    y = alt.Y(\"QLOW_DIFF:Q\"),\n",
    "    color=alt.condition(\n",
    "        alt.datum.QLOW_DIFF > 0,\n",
    "        alt.value(\"steelblue\"),  # The positive color\n",
    "        alt.value(\"orange\")  # The negative color\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9bb36e-ec88-4ad6-8004-2968c548214d",
   "metadata": {
    "collapsed": false,
    "name": "cell34"
   },
   "source": "### Conclusión\npandas en Snowflake libera el poder de Snowflake para los desarrolladores de pandas al permitirte ejecutar la misma API de pandas, mientras operas con grandes conjuntos de datos que típicamente no funcionan con pandas nativo y, ¡todo ello manteniendo tus datos en Snowflake! Para obtener más información, consulta la [Documentación de Snowflake](https://docs.snowflake.com/developer-guide/snowpark/python/snowpark-pandas). Para un ejemplo más avanzado, consulta [este inicio rápido](https://quickstarts.snowflake.com/guide/data_engineering_pipelines_with_snowpark_pandas/) sobre cómo puedes construir un pipeline de ingeniería de datos con Snowpark pandas."
  }
 ]
}
